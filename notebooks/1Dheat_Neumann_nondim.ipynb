{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1Dheat_systematic_weights.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-0AQRvfhI91",
        "colab_type": "text"
      },
      "source": [
        "# PINN for nondimenionalized heat equation\n",
        "\n",
        "First, check if GPU hardware acceleration is selected in \"Runtime\" -> \"Change runtime type\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XkbF-bvDY82",
        "colab_type": "text"
      },
      "source": [
        "# mount google drive (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k81z3ScvDYJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8zHw1n0Hsaw",
        "colab_type": "text"
      },
      "source": [
        "if you want to access/store files directly in your google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A6mM4x_D480",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if os.getcwd() != '/content/drive/My Drive/Colab Notebooks/PINNs':\n",
        "  os.chdir('/content/drive/My Drive/Colab Notebooks/PINNs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO6obp4wsJBr",
        "colab_type": "text"
      },
      "source": [
        "# download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WsfE5QrvjQt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b84d117d-5e06-4ac1-ce22-c3f0ad7edcd0"
      },
      "source": [
        "!wget https://github.com/mjokeit/PINN_heat/raw/master/continuous_time_inference/data/heat1D_nondim.mat"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-31 15:47:20--  https://github.com/mjokeit/PINN_heat/raw/master/continuous_time_inference/data/heat1D_nondim.mat\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mjokeit/PINN_heat/master/continuous_time_inference/data/heat1D_nondim.mat [following]\n",
            "--2020-07-31 15:47:20--  https://raw.githubusercontent.com/mjokeit/PINN_heat/master/continuous_time_inference/data/heat1D_nondim.mat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 820537 (801K) [application/octet-stream]\n",
            "Saving to: ‘heat1D_nondim.mat’\n",
            "\n",
            "heat1D_nondim.mat   100%[===================>] 801.31K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-07-31 15:47:21 (22.0 MB/s) - ‘heat1D_nondim.mat’ saved [820537/820537]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7FHUNsWr-sk",
        "colab_type": "text"
      },
      "source": [
        "# install pyDOE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r8rE549o30P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9dc41214-b538-46c0-faed-7c414c9cf632"
      },
      "source": [
        "!pip install -q pyDOE"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYJGchoCmfQ-",
        "colab_type": "text"
      },
      "source": [
        "# import plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtEDSPscmSxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Oct  9 20:11:57 2017\n",
        "\n",
        "@author: mraissi\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "#mpl.use('pgf')\n",
        "\n",
        "def figsize(scale, nplots = 1):\n",
        "    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n",
        "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
        "    golden_mean =  1/0.707 # (np.sqrt(5.0)-1.0)/2.0       # Aesthetic ratio (you could change this)\n",
        "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
        "    fig_height = nplots*fig_width*golden_mean              # height in inches\n",
        "    fig_size = [fig_width,fig_height]\n",
        "    return fig_size\n",
        "\n",
        "pgf_with_latex = {                      # setup matplotlib to use latex for output\n",
        "    \"pgf.texsystem\": \"pdflatex\",        # change this if using xetex or lautex\n",
        "    \"text.usetex\": True,                # use LaTeX to write all text\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [],                   # blank entries should cause plots to inherit fonts from the document\n",
        "    \"font.sans-serif\": [],\n",
        "    \"font.monospace\": [],\n",
        "    \"axes.labelsize\": 10,               # LaTeX default is 10pt font.\n",
        "    \"font.size\": 10,\n",
        "    \"legend.fontsize\": 8,               # Make the legend/label fonts a little smaller\n",
        "    \"xtick.labelsize\": 8,\n",
        "    \"ytick.labelsize\": 8,\n",
        "    \"figure.figsize\": figsize(1.0),     # default fig size of 0.9 textwidth\n",
        "    \"pgf.preamble\": [\n",
        "        r\"\\usepackage[utf8x]{inputenc}\",    # use utf8 fonts becasue your computer can handle it :)\n",
        "        r\"\\usepackage[T1]{fontenc}\",        # plots will be generated using this preamble\n",
        "        ]\n",
        "    }\n",
        "mpl.rcParams.update(pgf_with_latex)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# I make my own newfig and savefig functions\n",
        "def newfig(width, nplots = 1):\n",
        "    fig = plt.figure(figsize=figsize(width, nplots))\n",
        "    ax = fig.add_subplot(111)\n",
        "    return fig, ax\n",
        "\n",
        "def savefig(filename, crop = True):\n",
        "    if crop == True:\n",
        "        fig = plt.gcf()\n",
        "#        plt.savefig('{}.pgf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "        fig.savefig('{}.pdf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "        fig.savefig('{}.eps'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "    else:\n",
        "        fig = plt.gcf()\n",
        "#        plt.savefig('{}.pgf'.format(filename))\n",
        "        fig.savefig('{}.pdf'.format(filename))\n",
        "        fig.savefig('{}.eps'.format(filename))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbCIQPO0nOzP",
        "colab_type": "text"
      },
      "source": [
        "# PINN class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6cPZ6jnvaio",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c2d6640-7054-4b84-d264-1a6d1882028f"
      },
      "source": [
        "\"\"\"\n",
        "@author: Maziar Raissi\n",
        "@editor: Moritz Jokeit\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, '../utilities')\n",
        "\n",
        "# import os\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from pyDOE import lhs\n",
        "from plotting import newfig, savefig\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import time\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "\n",
        "\n",
        "class PhysicsInformedNN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, x0, u0, tb, X_f, X_f_val, layers, lb, ub, min_max_f, X_star, u_star):\n",
        "\n",
        "        X0 = np.concatenate((x0, 0 * x0), 1)  # (x0, 0)\n",
        "        X_lb = np.concatenate((0 * tb + lb[0], tb), 1)  # (lb[0], tb)\n",
        "        X_ub = np.concatenate((0 * tb + ub[0], tb), 1)  # (ub[0], tb)\n",
        "\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "\n",
        "        self.X_star = X_star\n",
        "        self.u_star = u_star\n",
        "\n",
        "        self.min_f = min_max_f[0]\n",
        "        self.max_f = min_max_f[1]\n",
        "\n",
        "        self.x0 = X0[:, 0:1]\n",
        "        self.t0 = X0[:, 1:2]\n",
        "\n",
        "        self.x_lb = X_lb[:, 0:1]\n",
        "        self.t_lb = X_lb[:, 1:2]\n",
        "\n",
        "        self.x_ub = X_ub[:, 0:1]\n",
        "        self.t_ub = X_ub[:, 1:2]\n",
        "\n",
        "        self.x_f = X_f[:, 0:1]\n",
        "        self.t_f = X_f[:, 1:2]\n",
        "\n",
        "        self.x_f_val = X_f_val[:, 0:1]\n",
        "        self.t_f_val = X_f_val[:, 1:2]\n",
        "\n",
        "        self.u0 = u0\n",
        "\n",
        "        self.log_var_u0 = tf.Variable(1.0, dtype='float32')\n",
        "        self.log_var_ub = tf.Variable(1.0, dtype='float32')\n",
        "        self.log_var_f = tf.Variable(1.0, dtype='float32')\n",
        "\n",
        "        # Initialize NNs\n",
        "        self.layers = layers\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "\n",
        "        # tf Placeholders\n",
        "        self.training = tf.placeholder(tf.bool)\n",
        "        self.penalties = tf.placeholder(tf.float32, shape=(3))\n",
        "\n",
        "        self.x0_tf = tf.placeholder(tf.float32, shape=[None, self.x0.shape[1]])\n",
        "        self.t0_tf = tf.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
        "\n",
        "        self.u0_tf = tf.placeholder(tf.float32, shape=[None, self.u0.shape[1]])\n",
        "\n",
        "        self.x_lb_tf = tf.placeholder(tf.float32, shape=[None, self.x_lb.shape[1]])\n",
        "        self.t_lb_tf = tf.placeholder(tf.float32, shape=[None, self.t_lb.shape[1]])\n",
        "\n",
        "        self.x_ub_tf = tf.placeholder(tf.float32, shape=[None, self.x_ub.shape[1]])\n",
        "        self.t_ub_tf = tf.placeholder(tf.float32, shape=[None, self.t_ub.shape[1]])\n",
        "\n",
        "        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, self.x_f.shape[1]])\n",
        "        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
        "\n",
        "        self.x_f_tf_val = tf.placeholder(tf.float32, shape=[None, self.x_f_val.shape[1]])\n",
        "        self.t_f_tf_val = tf.placeholder(tf.float32, shape=[None, self.t_f_val.shape[1]])\n",
        "\n",
        "        self.X_star_tf = tf.placeholder(tf.float32, shape=[None, self.X_star.shape[1]])\n",
        "\n",
        "        # tf Graphs\n",
        "        self.u0_pred, self.u0_x_pred = self.net_u(self.x0_tf, self.t0_tf)\n",
        "        self.u_lb_pred, self.u_x_lb_pred = self.net_u(self.x_lb_tf, self.t_lb_tf)\n",
        "        self.u_ub_pred, self.u_x_ub_pred = self.net_u(self.x_ub_tf, self.t_ub_tf)\n",
        "        self.f_pred = self.net_f(self.x_f_tf, self.t_f_tf)\n",
        "        self.s_pred = self.net_s(self.x_f_tf, self.t_f_tf)\n",
        "        self.f_pred_val = self.net_f(self.x_f_tf_val, self.t_f_tf_val)\n",
        "        self.s_pred_val = self.net_s(self.x_f_tf_val, self.t_f_tf_val)\n",
        "        self.u_pred, _ = self.net_u(self.X_star_tf[:, 0:1], self.X_star_tf[:, 1:2])\n",
        "\n",
        "        self.delta_u0 = tf.reduce_mean(self.u0_pred - self.u0_tf)\n",
        "\n",
        "        # MIN-MAX-SCALING BETWEEN [-sf, sf]\n",
        "        # sf = 1  # scale factor\n",
        "        # self.scaled_u0_tf = 2*sf * (self.u0_tf - tf.reduce_min(self.u0_tf)) / \\\n",
        "        #                       (tf.reduce_max(self.u0_tf) - tf.reduce_min(self.u0_tf)) - sf\n",
        "        # self.scaled_u0_pred = 2*sf * (self.u0_pred - tf.reduce_min(self.u0_tf)) / \\\n",
        "        #                       (tf.reduce_max(self.u0_tf) - tf.reduce_min(self.u0_tf)) - sf\n",
        "        # self.scaled_u_x_lb_pred = 2*sf*(self.u_x_lb_pred + 2e4) / (2e4 + 2e4) - sf\n",
        "        # self.scaled_u_x_ub_pred = 2*sf*(self.u_x_ub_pred + 2e4) / (2e4 + 2e4) - sf\n",
        "        # self.scaled_f_pred = 2*sf * (self.f_pred - self.min_f) / \\\n",
        "        #                      (self.max_f - self.min_f) - sf\n",
        "        # self.scaled_s_pred = 2*sf * (self.s_pred - self.min_f) / \\\n",
        "        #                      (self.max_f - self.min_f) - sf\n",
        "        # self.scaled_f_pred_val = self.min_max_scale(self.f_pred_val, self.min_f, self.max_f)\n",
        "        # self.scaled_s_pred_val = self.min_max_scale(self.s_pred_val, self.min_f, self.max_f)\n",
        "\n",
        "        # MAX ABS SCALING\n",
        "        abs_max_f = tf.cast(tf.reduce_max(tf.abs(min_max_f)), 'float32')\n",
        "        abs_max_u0 = tf.cast(tf.reduce_max(tf.abs(self.u0_tf)), 'float32')\n",
        "        self.scaled_u0_tf = self.u0_tf / abs_max_u0\n",
        "        self.scaled_u0_pred = self.u0_pred / abs_max_u0\n",
        "        self.scaled_u_x_lb_pred = self.u_x_lb_pred\n",
        "        self.scaled_u_x_ub_pred = self.u_x_ub_pred\n",
        "        self.scaled_f_pred = self.f_pred / abs_max_f\n",
        "        self.scaled_s_pred = self.s_pred / abs_max_f\n",
        "        self.scaled_f_pred_val = self.f_pred_val / abs_max_f\n",
        "        self.scaled_s_pred_val = self.s_pred_val / abs_max_f\n",
        "\n",
        "        # SCALED LOSSES FOR ADAPTIVE COST FUNCTION\n",
        "        self.loss_u0 = tf.reduce_mean(tf.square(self.scaled_u0_tf - self.scaled_u0_pred))\n",
        "        self.loss_ub = tf.reduce_mean(tf.square(self.scaled_u_x_lb_pred)) +\\\n",
        "                       tf.reduce_mean(tf.square(self.scaled_u_x_ub_pred))\n",
        "        self.loss_f = tf.reduce_mean(tf.square(self.scaled_f_pred - self.scaled_s_pred))\n",
        "        self.val_loss_f = tf.reduce_mean(tf.square(self.scaled_f_pred_val - self.scaled_s_pred_val))\n",
        "\n",
        "        # STANDARD LOSSES WITH OPTIONAL PENALTY FACTORS (penalties default to 1)\n",
        "        # ACTIVATE FOR OPTIONAL AUTO RESCALING AFTER FIRST ITERATIONS\n",
        "        # self.loss_u0 = self.penalties[0] * tf.reduce_mean(tf.square(self.u0_tf - self.u0_pred))\n",
        "        # self.loss_ub = self.penalties[1] * (tf.reduce_mean(tf.square(self.u_x_lb_pred)) +\n",
        "        #                                     tf.reduce_mean(tf.square(self.u_x_ub_pred)))\n",
        "        # self.loss_f = self.penalties[2] * tf.reduce_mean(tf.square(self.f_pred - self.s_pred))\n",
        "        # self.val_loss_f = tf.reduce_mean(tf.square(self.f_pred_val - self.s_pred_val))\n",
        "\n",
        "        # LOSS FORMULATION FOR AUTO ADAPTIVE LOSS (NOT STABLE)\n",
        "        # self.loss_u0 = tf.sqrt(tf.reduce_sum(tf.square(self.u0_tf - self.u0_pred)))\n",
        "        # self.loss_ub = tf.sqrt(tf.reduce_sum(tf.square(self.u_x_lb_pred)) + tf.reduce_sum(tf.square(self.u_x_ub_pred)))\n",
        "        # self.loss_f = tf.sqrt(tf.reduce_sum(tf.square(self.f_pred - self.s_pred)))\n",
        "        # self.val_loss_f = tf.reduce_mean(tf.square(self.f_pred_val - self.s_pred_val))\n",
        "\n",
        "        self.loss = self.loss_u0 + self.loss_ub + self.loss_f\n",
        "\n",
        "        # ALTERNATIVE LOSS FORMULATIONS\n",
        "        # self.loss = tf.log(self.loss_u0 +1) + tf.log(self.loss_ub + 1) + tf.log(self.loss_f + 1) # TEST OF A SIMPLE LOG LOSS\n",
        "        # self.loss = self.adaptive_loss() # promising weighted loss approach https://arxiv.org/pdf/1705.07115.pdf\n",
        "\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss,\n",
        "                                                                method='L-BFGS-B',\n",
        "                                                                options={'maxiter': 50000,\n",
        "                                                                         'maxfun': 50000,\n",
        "                                                                         'maxcor': 50,\n",
        "                                                                         'maxls': 50,\n",
        "                                                                         'ftol': 1e0 * np.finfo(float).eps, # ftol\n",
        "                                                                         # 'gtol': 1e-12\n",
        "                                                                         })# change gtol\n",
        "\n",
        "        self.optimizer_Adam = tf.train.AdamOptimizer(0.001) # more data, higher learning rate\n",
        "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "\n",
        "        # tf session\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "        self.train_history = []\n",
        "        self.error_u_history = []\n",
        "\n",
        "    # FIRST IMPLEMENTATION OF AUTO ADAPTIVE LOSS (NOT STABLE)\n",
        "    # def adaptive_loss(self):\n",
        "    #     pre_u0 = tf.exp(-self.log_var_u0)\n",
        "    #     pre_ub = tf.exp(-self.log_var_ub)\n",
        "    #     pre_f = tf.exp(-self.log_var_f)\n",
        "    #     loss = pre_u0*self.loss_u0 + pre_ub*self.loss_ub + pre_f*self.loss_f + \\\n",
        "    #            self.log_var_u0 + self.log_var_ub + self.log_var_f\n",
        "    #     return loss\n",
        "\n",
        "    def initialize_NN(self, layers):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    # GLOROT NORMAL INITIALIZATION\n",
        "    # def xavier_init(self, size):\n",
        "    #     in_dim = size[0]\n",
        "    #     out_dim = size[1]\n",
        "    #     xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "    #     return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "    # GLOROT UNIFORM INITIALIZATION\n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        limit = np.sqrt(6 / (in_dim + out_dim))\n",
        "        return tf.Variable(tf.random_uniform([in_dim, out_dim], -limit, limit), dtype=tf.float32)\n",
        "\n",
        "    def neural_net(self, X, weights, biases):\n",
        "        num_layers = len(weights) + 1\n",
        "\n",
        "        # H = (X - self.lb) / (self.ub - self.lb)                       # INPUT SCALING FOR SWISH / SQUARED RELU\n",
        "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0             # STANDARD MIN-MAX INPUT SCALING\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))                     # TANH ACTIVATION\n",
        "            # H = tf.square(tf.nn.relu(tf.add(tf.matmul(H, W), b)))     # SQUARED RELU ACTIVATION\n",
        "            # H = tf.sin(tf.add(tf.matmul(H, W), b))                    # SINE ACTIVATION\n",
        "            # H = tf.nn.swish(tf.add(tf.matmul(H, W), b))               # SWISH ACTIVATION\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.sinh(tf.add(tf.matmul(H, W), b))                         # SINH OUTPUT ACTIVATION\n",
        "        # Y = tf.add(tf.matmul(H, W), b)                                # LINEAR OUTPUT ACTIVATION\n",
        "        return Y\n",
        "\n",
        "    def net_u(self, x, t):\n",
        "        X = tf.concat([x, t], 1)\n",
        "        u = self.neural_net(X, self.weights, self.biases)\n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        return u, u_x\n",
        "\n",
        "    def net_f(self, x, t):\n",
        "        # computations for the lhs\n",
        "        u, u_x = self.net_u(x, t)\n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "\n",
        "        u_max = 800\n",
        "        us = u_max\n",
        "\n",
        "        k = (1.29 * 10 ** -2 * u*us + 6.856)\n",
        "        k_u = 1.29 * 10 ** -2 * us\n",
        "        k_x = k_u * u_x\n",
        "\n",
        "        c = (4.55 * 10 ** -4 * (u*us) ** 2 - 5.78 * 10 ** -3 * u*us + 5.849 * 10 ** 2)\n",
        "\n",
        "        f = c * u_t - k_x * u_x - k * u_xx\n",
        "\n",
        "        return f\n",
        "\n",
        "    def net_s(self, x, t):\n",
        "        t_max = 0.5\n",
        "        sigma = 0.02\n",
        "        u_max = 800\n",
        "\n",
        "        us = u_max\n",
        "\n",
        "        # computations for the rhs\n",
        "        p = 0.25 * tf.cos(2 * np.pi * t / t_max) + 0.5\n",
        "        p_t = tf.gradients(p, t)[0]\n",
        "\n",
        "        u_sol = u_max * tf.exp(-(x - p) ** 2 / (2 * sigma ** 2))\n",
        "\n",
        "        k_sol = 1.29 * 10 ** -2 * u_sol + 6.856\n",
        "        k_u_sol = 1.29 * 10 ** -2\n",
        "\n",
        "        c_sol = 4.55 * 10 ** -4 * u_sol ** 2 - 5.78 * 10 ** -3 * u_sol + 5.849 * 10 ** 2\n",
        "\n",
        "        s = 1/us * 1/sigma**2 * k_sol * u_sol + 1/us * u_sol * (x - p) * 1/sigma**2 * (\n",
        "                c_sol * p_t - (x - p) * 1/sigma**2 * (k_sol + u_sol * k_u_sol))\n",
        "\n",
        "        return s\n",
        "\n",
        "    def callback(self, loss, loss_u0, loss_ub, loss_f, val_loss_f, f_pred, scaled_f_pred, s_pred, scaled_s_pred, u_pred):\n",
        "        error_u = np.linalg.norm(self.u_star - u_pred, 2) / np.linalg.norm(self.u_star, 2)\n",
        "        print('f_pred: %.3e, scaled_f_pred: %.3e, s_pred: %.3e, scaled_s_pred: %.3e,' %\n",
        "              (np.max(f_pred), np.max(scaled_f_pred),\n",
        "               np.max(s_pred), np.max(scaled_s_pred)))\n",
        "        print('Loss: %.3e, Loss u0: %.3e, Loss ub: %.3e, Loss f: %.3e, Val. Loss f: %.3e' % (loss, loss_u0, loss_ub, loss_f, val_loss_f))\n",
        "        self.train_history.append([loss, loss_u0, loss_ub, loss_f, val_loss_f])\n",
        "        print('Error u: %e' % (error_u))\n",
        "        self.error_u_history.append(error_u)\n",
        "\n",
        "    def train(self, nIter):\n",
        "\n",
        "        tf_dict = {self.x0_tf: self.x0, self.t0_tf: self.t0,\n",
        "                   self.u0_tf: self.u0,\n",
        "                   self.x_lb_tf: self.x_lb, self.t_lb_tf: self.t_lb,\n",
        "                   self.x_ub_tf: self.x_ub, self.t_ub_tf: self.t_ub,\n",
        "                   self.x_f_tf: self.x_f, self.t_f_tf: self.t_f,\n",
        "                   self.x_f_tf_val: self.x_f_val, self.t_f_tf_val: self.t_f_val,\n",
        "                   self.X_star_tf: self.X_star,\n",
        "                   self.penalties: np.array([1.,1.,1.]),\n",
        "                   self.training: True}\n",
        "\n",
        "        # OPTIONAL AUTO SCALING BEFORE FIRST ITERATION\n",
        "        # loss_u0, loss_ub, loss_f = self.sess.run((self.loss_u0, self.loss_ub, self.loss_f), tf_dict)\n",
        "        # penalties = np.reciprocal(np.array([loss_u0, loss_ub, loss_f]))\n",
        "        # tf_dict[self.penalties] = penalties\n",
        "\n",
        "        # OPTIONAL MINI-BATCH TRAINING\n",
        "        #batch_size = 2000\n",
        "\n",
        "        start_time = time.time()\n",
        "        for it in range(nIter):\n",
        "            self.sess.run(self.train_op_Adam, tf_dict)\n",
        "            # for batch in range(int(N_f/batch_size)):\n",
        "            #     batch_idx = np.random.choice(self.x_f.shape[0], batch_size, replace=False)\n",
        "            #     tf_dict[self.x_f_tf] = self.x_f[batch_idx]\n",
        "            #     tf_dict[self.t_f_tf] = self.t_f[batch_idx]\n",
        "            #     self.sess.run(self.train_op_Adam, tf_dict)\n",
        "            #     # print(f'Batch no. {batch} finished')\n",
        "\n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value, loss_u0, loss_ub, loss_f, val_loss_f, u_pred = self.sess.run((self.loss, self.loss_u0, self.loss_ub, self.loss_f, self.val_loss_f, self.u_pred), tf_dict)\n",
        "                error_u = np.linalg.norm(self.u_star - u_pred, 2) / np.linalg.norm(self.u_star, 2)\n",
        "                print('It: %d, Loss: %.3e, Loss u0: %.3e, Loss ub: %.3e, Loss f: %.3e, Val. Loss f: %.3e, Time: %.2f' %\n",
        "                      (it, loss_value, loss_u0, loss_ub, loss_f, val_loss_f, elapsed))\n",
        "                print('Error u: %e' % (error_u))\n",
        "                self.error_u_history.extend(error_u for i in range(10))\n",
        "                self.train_history.extend([loss_value, loss_u0, loss_ub, loss_f, val_loss_f] for i in range(10))\n",
        "                start_time = time.time()\n",
        "\n",
        "        self.optimizer.minimize(self.sess,\n",
        "                                feed_dict=tf_dict,\n",
        "                                fetches=[self.loss, self.loss_u0, self.loss_ub,\n",
        "                                         self.loss_f, self.val_loss_f,\n",
        "                                         self.f_pred, self.scaled_f_pred,\n",
        "                                         self.s_pred, self.scaled_s_pred,\n",
        "                                         self.u_pred],\n",
        "                                loss_callback=self.callback)\n",
        "\n",
        "\n",
        "        return self.train_history, self.error_u_history\n",
        "\n",
        "    def predict(self, X_star):\n",
        "\n",
        "        tf_dict = {self.x0_tf: X_star[:, 0:1], self.t0_tf: X_star[:, 1:2], self.training: False}\n",
        "\n",
        "        u_star, u_x_star = self.sess.run((self.u0_pred, self.u0_x_pred), tf_dict)\n",
        "\n",
        "        tf_dict = {self.x_f_tf: X_star[:, 0:1], self.t_f_tf: X_star[:, 1:2], self.training: False}\n",
        "\n",
        "        f_star, s_star = self.sess.run((self.f_pred, self.s_pred), tf_dict)\n",
        "\n",
        "        W, b = self.sess.run((self.weights, self.biases), None)\n",
        "\n",
        "        num_layers = len(W)\n",
        "        max_W, max_b = [], []\n",
        "        for i in range(0, num_layers):\n",
        "            max_W.append(np.amax(np.abs(W[i])))\n",
        "            max_b.append(np.amax(np.abs(b[i])))\n",
        "\n",
        "        max_W = max(max_W)\n",
        "        max_b = max(max_b)\n",
        "\n",
        "        return u_star, u_x_star, f_star, s_star, max_W, max_b\n",
        "\n",
        "    def min_max_scale(self, X, X_min, X_max):\n",
        "        X_scaled = 2 * (X - X_min) / (X_max - X_min) - 1\n",
        "        return X_scaled"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMcRJzl0Vatg",
        "colab_type": "text"
      },
      "source": [
        "# define main loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufMF-zQdVe1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main_loop(loop_count, cwd): \n",
        "\n",
        "    noise = 0.0\n",
        "\n",
        "    N0 = 100\n",
        "    N_b = 50\n",
        "    N_f = 20000\n",
        "    N_f_val = 2000\n",
        "    layers = [2, 20, 20, 20, 1] # change layer structure\n",
        "\n",
        "    print(os.getcwd())\n",
        "\n",
        "    # data = scipy.io.loadmat('../../../data/heat1D_nondim.mat')\n",
        "    data = scipy.io.loadmat(cwd + '/heat1D_nondim.mat') # \n",
        "\n",
        "    t = data['t'].flatten()[:, None]\n",
        "    x = data['x'].flatten()[:, None]\n",
        "    Exact = data['usol_nondim'].T\n",
        "    Exact_flux = data['fluxsol_nondim'].T\n",
        "    min_max_f = data['min_max_f_nondim']\n",
        "\n",
        "    X, T = np.meshgrid(x, t)\n",
        "\n",
        "    X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
        "    u_star = Exact.flatten()[:, None]\n",
        "    flux_star = Exact_flux.flatten()[:, None]\n",
        "\n",
        "    # Domain bounds\n",
        "    lb = X_star.min(0)\n",
        "    ub = X_star.max(0)\n",
        "\n",
        "    ###########################\n",
        "\n",
        "    idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
        "    x0 = x[idx_x, :]\n",
        "    u0 = Exact.T[idx_x, 0:1]\n",
        "\n",
        "    idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
        "    tb = t[idx_t, :]\n",
        "\n",
        "    X_f = lb + (ub - lb) * lhs(2, N_f)\n",
        "    X_f_val = lb + (ub - lb) * lhs(2, N_f_val)\n",
        "\n",
        "    model = PhysicsInformedNN(x0, u0, tb, X_f, X_f_val, layers, lb, ub, min_max_f, X_star, u_star)\n",
        "\n",
        "    start_time = time.time()\n",
        "    # model.train(10000)\n",
        "    train_history, error_u_history = model.train(10000)\n",
        "    iterations = len(train_history)\n",
        "    print('Iterations: %d' % (iterations))\n",
        "    elapsed = time.time() - start_time\n",
        "    print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "    u_pred, u_x_pred, f_pred, s_pred, max_W, max_b = model.predict(X_star)\n",
        "    print(f'max_W: {max_W}, max_b: {max_b}')\n",
        "\n",
        "    # computing heat flux\n",
        "    u_max = 800\n",
        "    us = u_max\n",
        "    k = 1.29 * 10 ** -2 * u_pred + 6.856\n",
        "    # k = 6.856\n",
        "    flux_pred = -k * u_x_pred\n",
        "\n",
        "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "    print('Error u: %e' % (error_u))\n",
        "\n",
        "    error_flux = np.linalg.norm(flux_star - flux_pred, 2) / np.linalg.norm(flux_star, 2)\n",
        "    print('Error flux: %e' % (error_flux))\n",
        "\n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "    Error = np.abs(Exact - U_pred)\n",
        "\n",
        "    Flux_pred = griddata(X_star, flux_pred.flatten(), (X, T), method='cubic')\n",
        "    Error_flux = np.abs(Exact_flux - Flux_pred)\n",
        "\n",
        "    F_pred = griddata(X_star, f_pred.flatten(), (X, T), method='cubic')\n",
        "\n",
        "    ######################################################################\n",
        "    ############################# Plotting ###############################\n",
        "    ######################################################################\n",
        "\n",
        "    fig0, ax0 = newfig(1.0, 2)\n",
        "    ax0.axis('off')\n",
        "\n",
        "    ax0 = plt.subplot(2,1,1)\n",
        "    loss, loss_u0, loss_ub, loss_f, val_loss_f = ax0.plot(train_history)\n",
        "    ax0.set_yscale('log')\n",
        "    plt.legend([loss, loss_u0, loss_ub, loss_f, val_loss_f],\n",
        "               ['loss', 'loss u0', 'loss ub', 'loss f', 'val loss f'])\n",
        "\n",
        "    ax0 = plt.subplot(2, 1, 2)\n",
        "    error_u_plot = ax0.plot(error_u_history, label='error u')\n",
        "    ax0.set_yscale('log')\n",
        "    plt.legend()\n",
        "    savefig(f'1Dheat_Neumann_nondim_losses_error_{loop_count}')\n",
        "\n",
        "    X0 = np.concatenate((x0, 0 * x0), 1)  # (x0, 0)\n",
        "    X_lb = np.concatenate((0 * tb + lb[0], tb), 1)  # (lb[0], tb)\n",
        "    X_ub = np.concatenate((0 * tb + ub[0], tb), 1)  # (ub[0], tb)\n",
        "    X_u_train = np.vstack([X0, X_lb, X_ub])\n",
        "\n",
        "    fig, ax = newfig(1.0, 3)\n",
        "    ax.axis('off')\n",
        "\n",
        "    ####### Row 0: u(t,x) ##################\n",
        "    gs0 = gridspec.GridSpec(1, 2)\n",
        "    gs0.update(top=1 - 0.06, bottom=1 - 1 / 4, left=0.15, right=0.85, wspace=0)\n",
        "    ax = plt.subplot(gs0[:, :])\n",
        "\n",
        "    h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow',\n",
        "                  extent=[t.min(), t.max(), x.min(), x.max()],\n",
        "                  origin='lower', aspect='auto')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    fig.colorbar(h, cax=cax)\n",
        "\n",
        "    # plot data points used for training as 'x'\n",
        "    ax.plot(X_u_train[:, 1], X_u_train[:, 0], 'kx', label='data (%d points)' % (X_u_train.shape[0]), markersize=4,\n",
        "            clip_on=False)\n",
        "\n",
        "    # white lines on upper plot\n",
        "    line = np.linspace(x.min(), x.max(), 2)[:, None]\n",
        "    ax.plot(t[50] * np.ones((2, 1)), line, 'w-', linewidth=1)\n",
        "    # ax.plot(t[25] * np.ones((2, 1)), line, 'w-', linewidth=1)\n",
        "    ax.plot(t[100] * np.ones((2, 1)), line, 'w-', linewidth=1)\n",
        "    ax.plot(t[150] * np.ones((2, 1)), line, 'w-', linewidth=1)\n",
        "\n",
        "    # labels and legend for upper plot\n",
        "    ax.set_xlabel('$t$')\n",
        "    ax.set_ylabel('$x$')\n",
        "    ax.legend(frameon=False, loc='best')\n",
        "    ax.set_title('$u(t,x)$', fontsize=10)\n",
        "\n",
        "    ####### Row 1: phi(t,x) ##################\n",
        "    gs1 = gridspec.GridSpec(1, 2)\n",
        "    gs1.update(top=1 - 1 / 4 - 0.06, bottom=1 - 1 / 2, left=0.15, right=0.85, wspace=0)\n",
        "    ax = plt.subplot(gs1[:, :])\n",
        "\n",
        "    h = ax.imshow(Flux_pred.T, interpolation='nearest', cmap='rainbow',\n",
        "                  extent=[t.min(), t.max(), x.min(), x.max()],\n",
        "                  origin='lower', aspect='auto')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    fig.colorbar(h, cax=cax)\n",
        "\n",
        "    # plot data points used for training as 'x'\n",
        "    ax.plot(X_u_train[:, 1], X_u_train[:, 0], 'kx', label='data (%d points)' % (X_u_train.shape[0]), markersize=4,\n",
        "            clip_on=False)\n",
        "\n",
        "    # white lines on upper plot\n",
        "    line = np.linspace(x.min(), x.max(), 2)[:, None]\n",
        "    ax.plot(t[50] * np.ones((2, 1)), line, 'w-', linewidth=1)\n",
        "    # ax.plot(t[25] * np.ones((2, 1)), line, 'w-', linewidth=1)\n",
        "    ax.plot(t[100] * np.ones((2, 1)), line, 'w-', linewidth=1)\n",
        "    ax.plot(t[150] * np.ones((2, 1)), line, 'w-', linewidth=1)\n",
        "\n",
        "    # labels and legend for upper plot\n",
        "    ax.set_xlabel('$t$')\n",
        "    ax.set_ylabel('$x$')\n",
        "    ax.legend(frameon=False, loc='best')\n",
        "    ax.set_title('$\\phi(t,x)$', fontsize=10)\n",
        "\n",
        "    ####### Row 2: u(t,x) slices ##################\n",
        "    gs2 = gridspec.GridSpec(1, 3)\n",
        "    gs2.update(top=1 - 1 / 2, bottom=1 - 3 / 4, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "    ax = plt.subplot(gs2[0, 0])\n",
        "    ax.plot(x, Exact[50, :], 'b-', linewidth=2, label='Exact')\n",
        "    ax.plot(x, U_pred[50, :], 'r--', linewidth=2, label='Prediction')\n",
        "    # ax.plot(x, Exact[25, :], 'b-', linewidth=2, label='Exact')\n",
        "    # ax.plot(x, U_pred[25, :], 'r--', linewidth=2, label='Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.set_title('$t = 0.125$', fontsize=10)\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([lb[0] - 0.1, ub[0] + 0.1])\n",
        "    ax.set_ylim([Exact.min() - 0.1, Exact.max() + 0.1])\n",
        "\n",
        "    ax = plt.subplot(gs2[0, 1])\n",
        "    ax.plot(x, Exact[100, :], 'b-', linewidth=2, label='Exact')\n",
        "    ax.plot(x, U_pred[100, :], 'r--', linewidth=2, label='Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([lb[0] - 0.1, ub[0] + 0.1])\n",
        "    ax.set_ylim([Exact.min() - 0.1, Exact.max() + 0.1])\n",
        "    ax.set_title('$t = 0.25$', fontsize=10)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "    ax = plt.subplot(gs2[0, 2])\n",
        "    ax.plot(x, Exact[150, :], 'b-', linewidth=2, label='Exact')\n",
        "    ax.plot(x, U_pred[150, :], 'r--', linewidth=2, label='Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([lb[0] - 0.1, ub[0] + 0.1])\n",
        "    ax.set_ylim([Exact.min() - 0.1, Exact.max() + 0.1])\n",
        "    ax.set_title('$t = 0.375$', fontsize=10)\n",
        "\n",
        "    ####### Row 3: phi(t,x) slices ##################\n",
        "    gs3 = gridspec.GridSpec(1, 3)\n",
        "    gs3.update(top=1 - 3 / 4, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "    ax = plt.subplot(gs3[0, 0])\n",
        "    ax.plot(x, Exact_flux[50, :], 'b-', linewidth=2, label='Exact')\n",
        "    ax.plot(x, Flux_pred[50, :], 'r--', linewidth=2, label='Prediction')\n",
        "    # ax.plot(x, Exact[25, :], 'b-', linewidth=2, label='Exact')\n",
        "    # ax.plot(x, U_pred[25, :], 'r--', linewidth=2, label='Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$\\phi(t,x)$')\n",
        "    ax.set_title('$t = 0.125$', fontsize=10)\n",
        "    # ax.axis('square')\n",
        "    ax.set_xlim([lb[0] - 0.1, ub[0] + 0.1])\n",
        "    ax.set_ylim([Exact_flux.min() * (1 + 0.1), Exact_flux.max() * (1 + 0.1)])\n",
        "\n",
        "    ax = plt.subplot(gs3[0, 1])\n",
        "    ax.plot(x, Exact_flux[100, :], 'b-', linewidth=2, label='Exact')\n",
        "    ax.plot(x, Flux_pred[100, :], 'r--', linewidth=2, label='Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$\\phi(t,x)$')\n",
        "    # ax.axis('square')\n",
        "    ax.set_xlim([lb[0] - 0.1, ub[0] + 0.1])\n",
        "    ax.set_ylim([Exact_flux.min() * (1 + 0.1), Exact_flux.max() * (1 + 0.1)])\n",
        "    ax.set_title('$t = 0.25$', fontsize=10)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "    ax = plt.subplot(gs3[0, 2])\n",
        "    ax.plot(x, Exact_flux[150, :], 'b-', linewidth=2, label='Exact')\n",
        "    ax.plot(x, Flux_pred[150, :], 'r--', linewidth=2, label='Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$\\phi(t,x)$')\n",
        "    # ax.axis('square')\n",
        "    ax.set_xlim([lb[0] - 0.1, ub[0] + 0.1])\n",
        "    ax.set_ylim([Exact_flux.min() * (1 + 0.1), Exact_flux.max() * (1 + 0.1)])\n",
        "    ax.set_title('$t = 0.375$', fontsize=10)\n",
        "\n",
        "    savefig('1Dheat_Neumann_nondim', crop=False)\n",
        "    plt.show()\n"
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}